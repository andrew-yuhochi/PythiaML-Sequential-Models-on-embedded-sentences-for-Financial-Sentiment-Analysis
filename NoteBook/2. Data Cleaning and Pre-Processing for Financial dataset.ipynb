{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fossil-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "phrasebank_path = \"./raw_data/Financial phrasebank.csv\"\n",
    "df_finphrasebank = pd.read_csv(phrasebank_path, header = None)\n",
    "df_finphrasebank.columns = [\"label\", \"text\"]\n",
    "df_finphrasebank.label = df_finphrasebank.label.replace(['positive','neutral', 'negative'],[1, 0, -1])\n",
    "df_finphrasebank.text = df_finphrasebank.text.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "shared-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(text_list):\n",
    "    label = text_list[0][\"sentiment_score\"]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pediatric-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_train = pd.read_json(\"./raw_data/FiQA Task 1_post_train.json\", orient = \"index\")\n",
    "df_hl_train = pd.read_json(\"./raw_data/FiQA Task 1_headline_train.json\", orient = \"index\")\n",
    "df_fiqa = pd.concat([df_post_train, df_hl_train], ignore_index = True)\n",
    "df_fiqa[\"label\"] = df_fiqa[\"info\"].apply(extract_score)\n",
    "df_fiqa.columns = [\"text\", \"_\", \"label\"]\n",
    "df_fiqa = df_fiqa[[\"label\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "compatible-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import nltk\n",
    "\n",
    "#create global variables for data cleaning\n",
    "#punctuation that going to remove\n",
    "p = string.punctuation \n",
    "keep_p = [\"!\", \"\\\"\", \"#\", \"&\", \"\\'\", \",\", \".\", \":\", \";\", \"?\", \"@\"] # These is to keep/ useful to remove speical pattern later\n",
    "for i in keep_p:\n",
    "    p = p.replace(i, \"\") \n",
    "\n",
    "#speical word to be removed\n",
    "sp_word = [\"^rt$\", \"^#.*\",\"^@.*\", \"^http.*\", \"^www\\..*\", \"^&.*\", \"^;d$\", \"^;D$\", \"^;p$\", \"^;P$\", \"^:d$\", \"^:D$\",\n",
    "           \"^:p$\", \"^:P$\", \"^xd$\", \"^xp$\", \"^XD$\", \"^XP$\", \"^xD$\", \"^xP$\", \"^Xd$\", \"^Xp$\"]\n",
    "\n",
    "def data_cleaning(sentence):\n",
    "    \n",
    "    # react with special pattern discovered in pervious step\n",
    "    sentence = re.sub('!+', \"!\", sentence)\n",
    "    sentence = re.sub('\\\"+', \"\\\"\", sentence)\n",
    "    sentence = re.sub('#+', \" #\", sentence)\n",
    "    sentence = re.sub('&+', \" &\", sentence)\n",
    "    sentence = re.sub('\\'+', \"'\", sentence)\n",
    "    sentence = re.sub(' \\'+', \"'\", sentence)\n",
    "    sentence = re.sub(',+', \",\", sentence)\n",
    "    sentence = re.sub('\\.+', \".\", sentence)\n",
    "    sentence = re.sub(':+', \":\", sentence)\n",
    "    sentence = re.sub(';+', \";\", sentence)\n",
    "    sentence = re.sub('\\?+', \"?\", sentence)\n",
    "    sentence = re.sub('@+', \" @\", sentence)\n",
    "    sentence = re.sub('(\\?!)+', \"?!\", sentence)\n",
    "    sentence = re.sub('http', \" http\", sentence)\n",
    "    sentence = re.sub('www\\.', \" www.\", sentence)\n",
    "    \n",
    "    # Filter out the word did't in the punctuation removal list\n",
    "    sentence = \"\".join([j for j in sentence if j not in p]) \n",
    "    \n",
    "    # remove words in special pattern lists\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        for j in sp_word:\n",
    "            words[i] = re.sub(j, \"\", words[i])\n",
    "    \n",
    "    # join back the sentence\n",
    "    cleaned_text = \" \".join([k for k in words])\n",
    "    \n",
    "    # remove duplicate space\n",
    "    cleaned_text = re.sub(' +', \" \", cleaned_text)\n",
    "    return cleaned_text \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#stop word to be removed\n",
    "stop_words = stopwords.words('english')\n",
    "keep_w = ['against', 'up', 'down', 'over', 'under', 'no', 'not', 'same', 'above', 'below', 'only']\n",
    "for i in keep_w:\n",
    "     stop_words.remove(i) #inplace function\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    '''\n",
    "    This function convert nltk POS tagging to wordnet tagging. \n",
    "    Will be used inside data_lemmatization function\n",
    "    input: \n",
    "        nltk_tag: the nltk_tag that tagged for a particular word with nltk package\n",
    "    output:\n",
    "        wordnet tagging or NONE if no corresponding tagging found\n",
    "    '''\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def data_preprocessing(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    sentence = \" \".join([k for k in words])\n",
    "    \n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # create tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token without changing\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "metric-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finphrasebank.text = df_finphrasebank.text.apply(data_cleaning)\n",
    "df_finphrasebank.text = df_finphrasebank.text.apply(data_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "industrial-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fiqa.text = df_fiqa.text.apply(data_cleaning)\n",
    "df_fiqa.text = df_fiqa.text.apply(data_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-patio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
